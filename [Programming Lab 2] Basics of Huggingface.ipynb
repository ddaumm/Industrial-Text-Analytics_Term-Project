{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "417853a6",
   "metadata": {},
   "source": [
    "# Introduction: Programming Lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133ea31b",
   "metadata": {},
   "source": [
    "## Basics of Huggingface\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š **ê³¼ëª© ì •ë³´**\n",
    "- **ê³¼ëª©ëª…**: ì‚°ì—…í…ìŠ¤íŠ¸ ì• ë„ë¦¬í‹±ìŠ¤ (Industrial Text Analytics)\n",
    "- **í•™ê³¼**: ì„œìš¸ëŒ€í•™êµ ì‚°ì—…ê³µí•™ê³¼\n",
    "- **êµìˆ˜**: ê°•í•„ì„± êµìˆ˜ë‹˜\n",
    "- **ë‹´ë‹¹ì¡°êµ**: ì²œì¬ì›\n",
    "\n",
    "### ğŸ¯ **ì‹¤ìŠµ ê°œìš”**\n",
    "- **ì‹¤ìŠµ ì œëª©**: Basics of Huggingface\n",
    "- **ì‹¤ìŠµ í‚¤ì›Œë“œ**: \n",
    "  - Open-source Large Language Model (LLM)\n",
    "  - Huggingface Transformers\n",
    "  - Natural Language Processing (NLP)\n",
    "\n",
    "### ğŸ“‹ **ì‹¤ìŠµ ëª©í‘œ**\n",
    "- Huggingface Transformersì˜ ê¸°ë³¸ ê°œë…ê³¼ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¥¼ ì´í•´\n",
    "- ë‹¤ì–‘í•œ NLP ëª¨ë¸(Encoder / Decoder)ì„ ë‹¤ë£° ìˆ˜ ìˆëŠ” ê¸°ë°˜ í•¨ì–‘\n",
    "- Huggingface Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ êµ¬ì¡°ì™€ ì‘ë™ ì›ë¦¬ë¥¼ ìµí˜€, ì´í›„ Fine-tuningì´ë‚˜ ì»¤ìŠ¤í…€ í•™ìŠµìœ¼ë¡œ í™•ì¥(ì‹¤ìŠµ Part. 3, 4) ì¤€ë¹„\n",
    "- Tokenizer, ëª¨ë¸ í´ë˜ìŠ¤, Forward ë“± íŒŒì´í”„ë¼ì¸ì˜ í•µì‹¬ ë‹¨ê³„ë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ë©° ì´í•´\n",
    "\n",
    "\n",
    "### ğŸ—“ï¸ **ì‹¤ìŠµ ê³„íš**\n",
    "1. Huggingface ê°œìš”\n",
    "- Huggingfaceì™€ Open-source LLM ìƒíƒœê³„ì˜ ìœ„ì¹˜ ë° ì—­í• \n",
    "- Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ê´€ë ¨ ì„œë¸Œ íŒ¨í‚¤ì§€(datasets, evaluate ë“±) ì†Œê°œ\n",
    "2. ê¸°ë³¸ ì½”ë“œ êµ¬ì¡° ì´í•´\n",
    "- ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "- ì‚¬ì „ í•™ìŠµ ëª¨ë¸ê³¼ ì²´í¬í¬ì¸íŠ¸ ê°œë…\n",
    "3. Tokenizer ì‚¬ìš©ë²•\n",
    "- í† í¬ë‚˜ì´ì§• ê³¼ì • ë° ì£¼ìš” ì˜µì…˜\n",
    "- í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì˜ ì—°ê²° ë°©ì‹\n",
    "4. ëŒ€í‘œì ì¸ ëª¨ë¸ í´ë˜ìŠ¤ ì†Œê°œ\n",
    "- SequenceClassification, CausalLM ë“± íƒœìŠ¤í¬ë³„ í´ë˜ìŠ¤ êµ¬ì¡°\n",
    "- forward ë©”ì„œë“œì˜ ì£¼ìš” íŒŒë¼ë¯¸í„°ì™€ ë°˜í™˜ê°’ ë¶„ì„\n",
    "5. ì¶”ë¡  ë° ê²°ê³¼ í™•ì¸\n",
    "- ë¡œì§“(logits) í•´ì„ ë° í›„ì²˜ë¦¬(softmax, argmax ë“±)\n",
    "\n",
    "### ğŸ“– **ì°¸ê³  ìë£Œ**\n",
    "- Huggingface ê³µì‹ ë¬¸ì„œ(https://huggingface.co/docs)\n",
    "- Transformers GitHub Repository(https://github.com/huggingface/transformers)\n",
    "- Huggingface Course(https://huggingface.co/learn/llm-course/chapter1/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddb9bae",
   "metadata": {},
   "source": [
    "# **í™˜ê²½ ì„¤ì •**\n",
    "ì‹¤ìŠµì„ ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê³  importí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5465110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install needed\n",
    "# %pip install transformers datasets evaluate accelerate scikit-learn -qU\n",
    "# %pip install -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525fb096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLI ì°½ì—ì„œ ë°˜ë“œì‹œ Huggingface ë¡œê·¸ì¸ í•„ìš”\n",
    "# hf auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec221ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "# ë²„ì „ í™•ì¸\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43644f0",
   "metadata": {},
   "source": [
    "# **ì‹¤ìŠµ ì‹œì‘**\n",
    "\n",
    "> **ğŸ’¡ ì°¸ê³ ì‚¬í•­**: \n",
    "> - ê° ì…€ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•´ì£¼ì„¸ìš”\n",
    "> - ì—ëŸ¬ê°€ ë°œìƒí•  ê²½ìš° ë‹´ë‹¹ ì¡°êµì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”\n",
    "> - ì‹¤ìŠµ ì¤‘ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ì£¼ì„¸ìš”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1_title",
   "metadata": {},
   "source": [
    "---\n",
    "# **1. Huggingface ê°œìš”**\n",
    "\n",
    "> **í•™ìŠµ ëª©í‘œ**:\n",
    "> - Huggingfaceì™€ Open-source LLM ìƒíƒœê³„ì˜ ìœ„ì¹˜ ë° ì—­í•  ì´í•´\n",
    "> - Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ê´€ë ¨ ì„œë¸Œ íŒ¨í‚¤ì§€ ì´í•´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hf_ecosystem",
   "metadata": {},
   "source": [
    "## 1.1 Huggingfaceì™€ Open-source LLM ìƒíƒœê³„\n",
    "\n",
    "### **Huggingfaceì˜ ì—­í• **\n",
    "\n",
    "**Huggingface**ëŠ” í˜„ì¬ Open-source LLM ìƒíƒœê³„ì˜ ì¤‘ì‹¬ì— ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "1. **ğŸ¤— Hub**: ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì˜ ì¤‘ì•™ ì €ì¥ì†Œ\n",
    "   - 500,000+ ê°œì˜ ëª¨ë¸\n",
    "   - 100,000+ ê°œì˜ ë°ì´í„°ì…‹\n",
    "   - Git ê¸°ë°˜ ë²„ì „ ê´€ë¦¬\n",
    "\n",
    "2. **ğŸ”§ Transformers Library**: í†µí•© API\n",
    "   - PyTorch, TensorFlow, JAX ì§€ì›\n",
    "   - ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¡œ ë‹¤ì–‘í•œ ëª¨ë¸ ì‚¬ìš©\n",
    "\n",
    "3. **ğŸŒ Community**: í™œë°œí•œ ì˜¤í”ˆì†ŒìŠ¤ ì»¤ë®¤ë‹ˆí‹°\n",
    "   - ì—°êµ¬ì, ê°œë°œì, ê¸°ì—…ì˜ í˜‘ì—…\n",
    "   - ìµœì‹  ì—°êµ¬ ê²°ê³¼ì˜ ë¹ ë¥¸ êµ¬í˜„\n",
    "\n",
    "### **LLM ìƒíƒœê³„ì—ì„œì˜ ìœ„ì¹˜**\n",
    "\n",
    "```\n",
    "ì—°êµ¬ ë…¼ë¬¸ â†’ Huggingface êµ¬í˜„ â†’ ì»¤ë®¤ë‹ˆí‹° ì‚¬ìš© â†’ ì‚°ì—… ì ìš©\n",
    "```\n",
    "\n",
    "- OpenAI (GPT-oss), Google (BERT, T5), Meta (LLaMA) ë“±ì˜ ëª¨ë¸ì„ í†µí•© ê´€ë¦¬\n",
    "- í‘œì¤€í™”ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ëª¨ë¸ ê°„ ì „í™˜ ìš©ì´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hf_packages",
   "metadata": {},
   "source": [
    "## 1.2 Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ê´€ë ¨ ì„œë¸Œ íŒ¨í‚¤ì§€\n",
    "\n",
    "HuggingfaceëŠ” ì—¬ëŸ¬ ìƒí˜¸ ì—°ê²°ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hf_libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface ìƒíƒœê³„ ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤\n",
    "print(\"ğŸ¤— Huggingface Ecosystem Libraries:\\n\")\n",
    "\n",
    "libraries = {\n",
    "    \"transformers\": \"ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ìœ„í•œ ë©”ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\",\n",
    "    \"datasets\": \"ë°ì´í„°ì…‹ ë¡œë”©ê³¼ ì²˜ë¦¬\",\n",
    "    \"evaluate\": \"ëª¨ë¸ í‰ê°€ ë©”íŠ¸ë¦­\",\n",
    "    \"accelerate\": \"ë¶„ì‚° í•™ìŠµ ë° mixed precision\",\n",
    "    \"hub\": \"ëª¨ë¸ í—ˆë¸Œ ì¸í„°í˜ì´ìŠ¤\",\n",
    "}\n",
    "\n",
    "for lib, desc in libraries.items():\n",
    "    print(f\"ğŸ“¦ {lib:15} : {desc}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê° ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°„ë‹¨ ì˜ˆì œ\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "print(\"1. Transformers - Pipeline ì˜ˆì œ:\")\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device='cpu')\n",
    "result = classifier(\"Huggingface makes NLP easy!\")\n",
    "print(f\"   ëª¨ë¸: {classifier.model.name_or_path}\")\n",
    "print(\"   ì…ë ¥: Huggingface makes NLP easy!\")\n",
    "print(f\"   ê²°ê³¼: {result}\\n\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "print(\"2. Datasets - ë°ì´í„°ì…‹ ë¡œë”© ì˜ˆì œ:\")\n",
    "# ì‘ì€ ìƒ˜í”Œ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"train[:5]\")\n",
    "print(f\"   ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}\")\n",
    "print(f\"   ì²« ë²ˆì§¸ ìƒ˜í”Œ: {dataset[2]}\\n\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "print(\"3. Evaluate - í‰ê°€ ë©”íŠ¸ë¦­ ì˜ˆì œ:\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "predictions = [0, 1, 1, 0]\n",
    "references = [0, 1, 0, 0]\n",
    "result = accuracy.compute(predictions=predictions, references=references)\n",
    "print(f\"   Accuracy: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hub_exploration",
   "metadata": {},
   "source": [
    "## 1.3 Huggingface Hub íƒìƒ‰\n",
    "\n",
    "Hubì—ì„œ ëª¨ë¸ì„ íƒìƒ‰í•˜ê³  ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ì‹œë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hub_api",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, list_models\n",
    "\n",
    "# Hub API ì´ˆê¸°í™”\n",
    "api = HfApi()\n",
    "\n",
    "# ì¸ê¸° ìˆëŠ” í•œêµ­ì–´ ëª¨ë¸ ê²€ìƒ‰\n",
    "print(\"ğŸ” í•œêµ­ì–´ BERT ëª¨ë¸ ê²€ìƒ‰:\\n\")\n",
    "korean_models = list(list_models(\n",
    "    filter=\"bert\",\n",
    "    search=\"korean\",\n",
    "    limit=5,\n",
    "    sort=\"downloads\",\n",
    "    direction=-1\n",
    "))\n",
    "\n",
    "for i, model in enumerate(korean_models, 1):\n",
    "    print(f\"{i}. {model.modelId}\")\n",
    "    print(f\"   Downloads: {model.downloads:,}\")\n",
    "    print(f\"   Likes: {model.likes}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2_title",
   "metadata": {},
   "source": [
    "---\n",
    "# **2. ê¸°ë³¸ ì½”ë“œ êµ¬ì¡° ì´í•´**\n",
    "\n",
    "> **í•™ìŠµ ëª©í‘œ**:\n",
    "> - ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë°©ë²• ì´í•´\n",
    "> - ì‚¬ì „ í•™ìŠµ ëª¨ë¸ê³¼ ì²´í¬í¬ì¸íŠ¸ ê°œë… ì´í•´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_loading_concept",
   "metadata": {},
   "source": [
    "## 2.1 ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°ì˜ ê¸°ë³¸ ê°œë…\n",
    "\n",
    "### **ì²´í¬í¬ì¸íŠ¸(Checkpoint)ë€?**\n",
    "\n",
    "- í•™ìŠµëœ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ì™€ ì„¤ì •ì´ ì €ì¥ëœ ìƒíƒœ\n",
    "- Hubì— ì—…ë¡œë“œëœ ëª¨ë¸ ì‹ë³„ì (ì˜ˆ: `bert-base-uncased`)\n",
    "- ë¡œì»¬ ê²½ë¡œì— ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ë“¤\n",
    "\n",
    "### **ëª¨ë¸ êµ¬ì„± ìš”ì†Œ**\n",
    "\n",
    "1. **Config**: ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ë³´ (`config.json`, `generation_config.json`)\n",
    "2. **Weights**: í•™ìŠµëœ ê°€ì¤‘ì¹˜ (`pytorch_model.bin`, `model-0000N-of-00002.safetensors`)\n",
    "3. **Tokenizer**: í† í¬ë‚˜ì´ì € íŒŒì¼ë“¤ (`tokenizer_config.json`, `vocab.txt` ë“±)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_loading_methods",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModel, \n",
    "    AutoTokenizer, \n",
    "    AutoConfig,\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    "    BertConfig\n",
    ")\n",
    "\n",
    "print(\"ğŸ”„ ëª¨ë¸ ë¡œë”© ë°©ë²• ë¹„êµ\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ë°©ë²• 1: Auto Classes (ê¶Œì¥)\n",
    "print(\"\\në°©ë²• 1: Auto Classes ì‚¬ìš© (ê¶Œì¥)\")\n",
    "model_name = \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "print(f\"âœ… Model type: {model.__class__.__name__}\")\n",
    "print(f\"âœ… Config type: {config.__class__.__name__}\")\n",
    "print(f\"âœ… Tokenizer type: {tokenizer.__class__.__name__}\")\n",
    "\n",
    "# ë°©ë²• 2: íŠ¹ì • ëª¨ë¸ í´ë˜ìŠ¤ ì§ì ‘ ì‚¬ìš©\n",
    "print(\"\\në°©ë²• 2: íŠ¹ì • ëª¨ë¸ í´ë˜ìŠ¤ ì§ì ‘ ì‚¬ìš©\")\n",
    "bert_config = BertConfig.from_pretrained(model_name)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "print(f\"âœ… Model type: {bert_model.__class__.__name__}\")\n",
    "print(f\"âœ… ë™ì¼í•œ ëª¨ë¸: {type(model) == type(bert_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint_details",
   "metadata": {},
   "source": [
    "## 2.2 ì²´í¬í¬ì¸íŠ¸ ìƒì„¸ ì •ë³´ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checkpoint_info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì„¤ì • ì •ë³´ í™•ì¸\n",
    "model_name = \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "print(\"ğŸ“‹ BERT ëª¨ë¸ ì„¤ì • (Config) ì •ë³´:\\n\")\n",
    "print(f\"ì•„í‚¤í…ì²˜ íƒ€ì…: {config.model_type}\")\n",
    "print(f\"íˆë“  í¬ê¸°: {config.hidden_size}\")\n",
    "print(f\"ë ˆì´ì–´ ìˆ˜: {config.num_hidden_layers}\")\n",
    "print(f\"ì–´í…ì…˜ í—¤ë“œ ìˆ˜: {config.num_attention_heads}\")\n",
    "print(f\"Vocabulary í¬ê¸°: {config.vocab_size}\")\n",
    "print(f\"ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {config.max_position_embeddings}\")\n",
    "print(f\"Dropout í™•ë¥ : {config.hidden_dropout_prob}\")\n",
    "print(f\"\\nì „ì²´ ì„¤ì • í‚¤: {list(config.to_dict().keys())[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬ì¡° í™•ì¸\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "print(\"ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬ì¡°:\\n\")\n",
    "print(f\"ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters():,}\")\n",
    "print(f\"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# ëª¨ë¸ì˜ ì£¼ìš” ì»´í¬ë„ŒíŠ¸\n",
    "print(\"\\nì£¼ìš” ì»´í¬ë„ŒíŠ¸:\")\n",
    "for name, module in model.named_children():\n",
    "    print(f\"- {name}: {module.__class__.__name__}\")\n",
    "    if hasattr(module, 'num_parameters'):\n",
    "        print(f\"  íŒŒë¼ë¯¸í„° ìˆ˜: {module.num_parameters():,}\")\n",
    "\n",
    "# ëª¨ë¸ì˜ ì „ì²´ ìƒì„¸ êµ¬ì¡°\n",
    "print(\"\\nëª¨ë¸ êµ¬ì¡°:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cache_management",
   "metadata": {},
   "source": [
    "## 2.3 ëª¨ë¸ ìºì‹±ê³¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cache_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸\n",
    "cache_dir = transformers.utils.hub.TRANSFORMERS_CACHE\n",
    "print(f\"ğŸ“ Transformers ìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}\\n\")\n",
    "\n",
    "# ëª¨ë¸ì„ íŠ¹ì • ë””ë ‰í† ë¦¬ì— ì €ì¥\n",
    "save_directory = \"./my_bert_model\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "print(\"1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ...\")\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# ë¡œì»¬ì— ì €ì¥\n",
    "print(f\"2. ëª¨ë¸ì„ '{save_directory}'ì— ì €ì¥...\")\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "# ì €ì¥ëœ íŒŒì¼ í™•ì¸\n",
    "print(f\"\\n3. ì €ì¥ëœ íŒŒì¼ë“¤:\")\n",
    "if os.path.exists(save_directory):\n",
    "    for file in os.listdir(save_directory):\n",
    "        file_path = os.path.join(save_directory, file)\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"   - {file}: {size_mb:.2f} MB\")\n",
    "\n",
    "# ë¡œì»¬ì—ì„œ ë‹¤ì‹œ ë¡œë“œ\n",
    "print(f\"\\n4. ë¡œì»¬ì—ì„œ ëª¨ë¸ ë¡œë“œ...\")\n",
    "local_model = AutoModel.from_pretrained(save_directory)\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "print(\"âœ… ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c110a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ë¦¬\n",
    "import shutil\n",
    "if os.path.exists(save_directory):\n",
    "    shutil.rmtree(save_directory)\n",
    "    print(f\"\\nğŸ§¹ ì„ì‹œ ë””ë ‰í† ë¦¬ '{save_directory}' ì‚­ì œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3_title",
   "metadata": {},
   "source": [
    "---\n",
    "# **3. Tokenizer ì‚¬ìš©ë²•**\n",
    "\n",
    "> **í•™ìŠµ ëª©í‘œ**:\n",
    "> - í† í¬ë‚˜ì´ì§• ê³¼ì • ë° ì£¼ìš” ì˜µì…˜ ì´í•´\n",
    "> - í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì˜ ì—°ê²° ë°©ì‹ ì´í•´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer_concept",
   "metadata": {},
   "source": [
    "## 3.1 í† í¬ë‚˜ì´ì§• ê³¼ì • ì´í•´\n",
    "\n",
    "### **í† í¬ë‚˜ì´ì§•ì˜ í•µì‹¬ ë‹¨ê³„**\n",
    "\n",
    "1. **Text Normalization**: í…ìŠ¤íŠ¸ ì •ê·œí™” (ì†Œë¬¸ì ë³€í™˜ ë“±)\n",
    "2. **Pre-tokenization**: ê³µë°± ê¸°ì¤€ ë¶„ë¦¬\n",
    "3. **Tokenization**: ì„œë¸Œì›Œë“œ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "4. **Post-processing**: íŠ¹ìˆ˜ í† í° ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenization_steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# ì˜ˆì œ í…ìŠ¤íŠ¸\n",
    "text = \"Tokenization is the first step in NLP!\"\n",
    "\n",
    "print(\"ğŸ”¤ í† í¬ë‚˜ì´ì§• ë‹¨ê³„ë³„ ë¶„ì„\\n\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ì›ë³¸ í…ìŠ¤íŠ¸: '{text}'\\n\")\n",
    "\n",
    "# Step 1: í† í°í™” (íŠ¹ìˆ˜ í† í° ì—†ì´)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Step 1 - í† í°í™”:\")\n",
    "print(f\"  í† í°: {tokens}\")\n",
    "print(f\"  í† í° ìˆ˜: {len(tokens)}\\n\")\n",
    "\n",
    "# Step 2: í† í°ì„ IDë¡œ ë³€í™˜\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Step 2 - í† í° â†’ ID ë³€í™˜:\")\n",
    "for token, id in zip(tokens, token_ids):\n",
    "    print(f\"  '{token}' â†’ {id}\")\n",
    "\n",
    "# Step 3: ì¸ì½”ë”© (íŠ¹ìˆ˜ í† í° í¬í•¨)\n",
    "print(\"\\nStep 3 - ì „ì²´ ì¸ì½”ë”© (íŠ¹ìˆ˜ í† í° í¬í•¨):\")\n",
    "encoded = tokenizer.encode(text, add_special_tokens=True)\n",
    "print(f\"  ì¸ì½”ë”©ëœ ID: {encoded}\")\n",
    "print(f\"  ê¸¸ì´: {len(encoded)}\")\n",
    "\n",
    "# íŠ¹ìˆ˜ í† í° í™•ì¸\n",
    "print(\"\\níŠ¹ìˆ˜ í† í°:\")\n",
    "print(f\"  [CLS] token ID: {tokenizer.cls_token_id}\")\n",
    "print(f\"  [SEP] token ID: {tokenizer.sep_token_id}\")\n",
    "\n",
    "# ë””ì½”ë”©\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"\\në””ì½”ë”© ê²°ê³¼: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer_options",
   "metadata": {},
   "source": [
    "## 3.2 í† í¬ë‚˜ì´ì € ì£¼ìš” ì˜µì…˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5e14ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ì–‘í•œ í† í¬ë‚˜ì´ì € ì˜µì…˜ ì‹¤ìŠµ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤ (ë‹¤ì–‘í•œ ê¸¸ì´)\n",
    "sentences = [\n",
    "    \"Short text.\",\n",
    "    \"This is a medium length sentence for testing tokenizer.\",\n",
    "    \"This is a very long sentence that will demonstrate how padding and truncation work in the tokenizer when we have different length inputs.\"\n",
    "]\n",
    "\n",
    "print(\"âš™ï¸ í† í¬ë‚˜ì´ì € ì˜µì…˜ ì‹¤ìŠµ\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenizer_options_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜µì…˜ 1: padding\n",
    "print(\"\\n1. Padding ì˜µì…˜:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# padding=True (ìµœëŒ€ ê¸¸ì´ì— ë§ì¶¤)\n",
    "encoded_padding = tokenizer(\n",
    "    sentences,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "for i, sent in enumerate(sentences):\n",
    "    print(f\"\\në¬¸ì¥ {i+1} (ê¸¸ì´: {len(sent.split())} ë‹¨ì–´)\")\n",
    "    print(f\"Input Tokens: {tokenizer.convert_ids_to_tokens(encoded_padding['input_ids'][i].tolist())}\")\n",
    "    print(f\"Input IDs: {encoded_padding['input_ids'][i].tolist()}\")\n",
    "    print(f\"Input IDs ê¸¸ì´: {len(encoded_padding['input_ids'][i])}\")\n",
    "    print(f\"Attention Mask: {encoded_padding['attention_mask'][i].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b358f3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜µì…˜ 2: truncation\n",
    "print(\"2. Truncation ì˜µì…˜:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "max_length = 10\n",
    "encoded_truncation = tokenizer(\n",
    "    sentences[2],  # ê¸´ ë¬¸ì¥ ì‚¬ìš©\n",
    "    truncation=True,\n",
    "    max_length=max_length,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(f\"ì›ë³¸ ë¬¸ì¥ í† í° ìˆ˜: {len(tokenizer.tokenize(sentences[2]))}\")\n",
    "print(f\"Truncation í›„ ê¸¸ì´: {len(encoded_truncation['input_ids'][0])}\")\n",
    "print(f\"Truncated IDs: {encoded_truncation['input_ids'][0].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜µì…˜ 3: return_tensors\n",
    "print(\"3. Return Tensors ì˜µì…˜:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# PyTorch tensors\n",
    "encoded_pt = tokenizer(sentences[0], return_tensors=\"pt\")\n",
    "print(f\"PyTorch tensor: {type(encoded_pt['input_ids'])}\")\n",
    "print(f\"Shape: {encoded_pt['input_ids'].shape}\")\n",
    "\n",
    "# NumPy arrays\n",
    "encoded_np = tokenizer(sentences[0], return_tensors=\"np\")\n",
    "print(f\"NumPy array: {type(encoded_np['input_ids'])}\")\n",
    "print(f\"Shape: {encoded_np['input_ids'].shape}\")\n",
    "\n",
    "# Python lists (default)\n",
    "encoded_list = tokenizer(sentences[0])\n",
    "print(f\"Python list: {type(encoded_list['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer_model_connection",
   "metadata": {},
   "source": [
    "## 3.3 í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì˜ ì—°ê²°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "print(\"ğŸ”— í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ì—°ê²°\\n\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842fa83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°™ì€ ì²´í¬í¬ì¸íŠ¸ì—ì„œ í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì˜ vocabulary í¬ê¸° í™•ì¸\n",
    "print(f\"í† í¬ë‚˜ì´ì € vocabulary í¬ê¸°: {tokenizer.vocab_size}\")\n",
    "print(f\"ëª¨ë¸ embedding í¬ê¸°: {model.config.vocab_size}\")\n",
    "print(f\"ì¼ì¹˜ ì—¬ë¶€: {tokenizer.vocab_size == model.config.vocab_size}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c477a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ\n",
    "text = [\"Understanding the connection between tokenizer and model.\", \n",
    "        \"Understanding the connection between tokenizer and model. \"*100]\n",
    "\n",
    "# í† í¬ë‚˜ì´ì§•\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=64\n",
    ")\n",
    "\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"í† í¬ë‚˜ì´ì € ì¶œë ¥:\")\n",
    "print(f\"  - input_ids shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"  - attention_mask shape: {inputs['attention_mask'].shape}\")\n",
    "print(f\"  - first sentence input_ids: {inputs['input_ids'][0]}\")\n",
    "print(f\"  - first sentence attention_mask: {inputs['attention_mask'][0]}\")\n",
    "print(f\"  - second sentence input_ids: {inputs['input_ids'][1]}\")\n",
    "print(f\"  - second sentence original token length: {len(tokenizer.tokenize(text[1]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenizer_model_sync",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ì— ì…ë ¥\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(outputs.keys())\n",
    "\n",
    "print(\"\\nëª¨ë¸ ì¶œë ¥:\")\n",
    "print(f\"  - last_hidden_state shape: {outputs.last_hidden_state.shape}\")\n",
    "print(f\"    (batch_size={outputs.last_hidden_state.shape[0]}, \")\n",
    "print(f\"     sequence_length={outputs.last_hidden_state.shape[1]}, \")\n",
    "print(f\"     hidden_size={outputs.last_hidden_state.shape[2]})\")\n",
    "print(f\"  - pooler_output shape: {outputs.pooler_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a99496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒí™©ì— ë”°ë¼ ë‹¤ì–‘í•œ ì¶œë ¥ì„ íšë“í•  ìˆ˜ ìˆìŒ\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "print(outputs.keys())\n",
    "\n",
    "print(\"\\nëª¨ë¸ ì¶œë ¥:\")\n",
    "print(f\"  - last_hidden_state shape: {outputs.last_hidden_state.shape}\")\n",
    "print(f\"    (batch_size={outputs.last_hidden_state.shape[0]}, \")\n",
    "print(f\"     sequence_length={outputs.last_hidden_state.shape[1]}, \")\n",
    "print(f\"     hidden_size={outputs.last_hidden_state.shape[2]})\")\n",
    "print(f\"  - pooler_output shape: {outputs.pooler_output.shape}\")\n",
    "print(f\"  - attentions shape: {outputs.attentions[0].shape}\")\n",
    "print(f\"  - hidden_states shape: {outputs.hidden_states[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fd8296",
   "metadata": {},
   "source": [
    "## 3.4 íŠ¹ìˆ˜ í† í° ì»¤ìŠ¤í„°ë§ˆì´ì§•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b403941",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# í˜„ì¬ íŠ¹ìˆ˜ í† í°ë“¤\n",
    "print(\"ê¸°ë³¸ íŠ¹ìˆ˜ í† í°:\")\n",
    "\n",
    "for name, token in tokenizer.special_tokens_map.items():\n",
    "    if token:\n",
    "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "        print(f\"  {name:4} token: '{token}' (ID: {token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a8477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì¥ ìŒ ì¸ì½”ë”© (BERTì˜ ê²½ìš°)\n",
    "print(\"\\në¬¸ì¥ ìŒ ì¸ì½”ë”© ì˜ˆì œ:\")\n",
    "sentence1 = \"This is the first sentence.\"\n",
    "sentence2 = \"This is the second sentence.\"\n",
    "\n",
    "# ë‘ ë¬¸ì¥ì„ í•¨ê»˜ ì¸ì½”ë”©\n",
    "encoded = tokenizer(\n",
    "    sentence1,\n",
    "    sentence2,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# token_type_ids í™•ì¸ (ë¬¸ì¥ êµ¬ë¶„)\n",
    "print(f\"\\nToken IDs: {encoded['input_ids'][0].tolist()}\")\n",
    "print(f\"Token Type IDs: {encoded['token_type_ids'][0].tolist()}\")\n",
    "print(f\"  â†’ 0: ì²« ë²ˆì§¸ ë¬¸ì¥, 1: ë‘ ë²ˆì§¸ ë¬¸ì¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special_tokens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë””ì½”ë”©í•´ì„œ í™•ì¸\n",
    "decoded = tokenizer.decode(encoded['input_ids'][0])\n",
    "print(f\"\\në””ì½”ë”© ê²°ê³¼: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4_title",
   "metadata": {},
   "source": [
    "---\n",
    "# **4. ëŒ€í‘œì ì¸ ëª¨ë¸ í´ë˜ìŠ¤ ì†Œê°œ**\n",
    "\n",
    "> **í•™ìŠµ ëª©í‘œ**:\n",
    "> - SequenceClassification, CausalLM ë“± íƒœìŠ¤í¬ë³„ í´ë˜ìŠ¤ êµ¬ì¡° ì´í•´\n",
    "> - forward ë©”ì„œë“œì˜ ì£¼ìš” íŒŒë¼ë¯¸í„°ì™€ ë°˜í™˜ê°’ ë¶„ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_classes_overview",
   "metadata": {},
   "source": [
    "## 4.1 íƒœìŠ¤í¬ë³„ ëª¨ë¸ í´ë˜ìŠ¤ ê°œìš”\n",
    "\n",
    "### **ì£¼ìš” ëª¨ë¸ í´ë˜ìŠ¤ë“¤**\n",
    "\n",
    "HuggingfaceëŠ” ê° íƒœìŠ¤í¬ì— íŠ¹í™”ëœ ëª¨ë¸ í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤:\n",
    "\n",
    "1. **AutoModel**: Base model (hidden statesë§Œ ë°˜í™˜)\n",
    "2. **AutoModelForSequenceClassification**: í…ìŠ¤íŠ¸ ë¶„ë¥˜\n",
    "3. **AutoModelForTokenClassification**: í† í° ë¶„ë¥˜ (NER, POS)\n",
    "4. **AutoModelForQuestionAnswering**: ì§ˆì˜ì‘ë‹µ\n",
    "5. **AutoModelForCausalLM**: í…ìŠ¤íŠ¸ ìƒì„± (GPTë¥˜)\n",
    "6. **AutoModelForMaskedLM**: Masked Language Modeling (BERTë¥˜)\n",
    "7. **AutoModelForSeq2SeqLM**: Sequence-to-Sequence (T5, BART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“š íƒœìŠ¤í¬ë³„ ëª¨ë¸ í´ë˜ìŠ¤ ë¹„êµ\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from transformers import (\n",
    "    # AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMaskedLM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_classes_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê° íƒœìŠ¤í¬ì— ì í•©í•œ ëª¨ë¸ ì˜ˆì‹œ\n",
    "model_examples = [\n",
    "    (\"Sequence Classification\", \"distilbert-base-uncased-finetuned-sst-2-english\", AutoModelForSequenceClassification),\n",
    "    (\"Token Classification\", \"dbmdz/bert-large-cased-finetuned-conll03-english\", AutoModelForTokenClassification),\n",
    "    (\"Question Answering\", \"distilbert-base-cased-distilled-squad\", AutoModelForQuestionAnswering),\n",
    "    (\"Causal LM\", \"gpt2\", AutoModelForCausalLM),\n",
    "    (\"Masked LM\", \"bert-base-uncased\", AutoModelForMaskedLM)\n",
    "]\n",
    "\n",
    "for task, model_name, model_class in model_examples:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"íƒœìŠ¤í¬: {task}\")\n",
    "    print(f\"ëª¨ë¸: {model_name}\")\n",
    "    print(f\"í´ë˜ìŠ¤: {model_class.__name__}\")\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ (ì‹¤ì œë¡œëŠ” ë¡œë“œí•˜ì§€ ì•Šê³  configë§Œ í™•ì¸)\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    \n",
    "    print(f\"ì•„í‚¤í…ì²˜: {config.architectures[0] if hasattr(config, 'architectures') else 'N/A'}\")\n",
    "    \n",
    "    # íƒœìŠ¤í¬ë³„ íŠ¹ìˆ˜ ì†ì„±\n",
    "    if hasattr(config, 'num_labels'):\n",
    "        print(f\"Number of labels: {config.num_labels}\")\n",
    "    if hasattr(config, 'id2label'):\n",
    "        print(f\"Labels: {list(config.id2label.values())[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sequence_classification_detail",
   "metadata": {},
   "source": [
    "## 4.2 SequenceClassification ëª¨ë¸ ìƒì„¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41b2825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ê°œìš”\n",
    "print(\"ëª¨ë¸ ê°œìš”:\")\n",
    "print(f\"- Base model: {model.base_model_prefix}\")\n",
    "print(f\"- Number of labels: {model.config.num_labels}\")\n",
    "print(f\"- Labels: {model.config.id2label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb08329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ìƒì„¸ êµ¬ì¡°\n",
    "print(\"ëª¨ë¸ ìƒì„¸ êµ¬ì¡°:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f195552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward ë©”ì„œë“œ íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸ (Positive ì˜ˆì‹œ)\n",
    "text = \"This movie is absolutely fantastic!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"ì…ë ¥ í…ìŠ¤íŠ¸: '{text}'\\n\")\n",
    "print(\"Forward ë©”ì„œë“œ ì…ë ¥ íŒŒë¼ë¯¸í„°:\")\n",
    "for key, value in inputs.items():\n",
    "    print(f\"  - {key}: shape {value.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64241fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶œë ¥ íŒŒë¼ë¯¸í„°\n",
    "print(outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5aa415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶œë ¥ ë¶„ì„\n",
    "print(\"\\nForward ë©”ì„œë“œ ì¶œë ¥:\")\n",
    "print(f\"  - logits: shape {outputs.logits.shape}\")\n",
    "print(f\"    ê°’: {outputs.logits[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338765be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward ë©”ì„œë“œ íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸ (Negative ì˜ˆì‹œ)\n",
    "text = \"This movie is totally awful!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"ì…ë ¥ í…ìŠ¤íŠ¸: '{text}'\\n\")\n",
    "print(\"Forward ë©”ì„œë“œ ì…ë ¥ íŒŒë¼ë¯¸í„°:\")\n",
    "for key, value in inputs.items():\n",
    "    print(f\"  - {key}: shape {value.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b523d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶œë ¥ ë¶„ì„\n",
    "print(\"\\nForward ë©”ì„œë“œ ì¶œë ¥:\")\n",
    "print(f\"  - logits: shape {outputs.logits.shape}\")\n",
    "print(f\"    ê°’: {outputs.logits[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seq_class_forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden states ì ‘ê·¼ (output_hidden_states=True)\n",
    "with torch.no_grad():\n",
    "    outputs_with_hidden = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "if outputs_with_hidden.hidden_states:\n",
    "    print(f\"\\n  - hidden_states: {len(outputs_with_hidden.hidden_states)} layers\")\n",
    "    print(f\"    ê° ë ˆì´ì–´ shape: {outputs_with_hidden.hidden_states[0].shape}\")\n",
    "\n",
    "# Attention weights ì ‘ê·¼ (output_attentions=True)\n",
    "with torch.no_grad():\n",
    "    outputs_with_attention = model(**inputs, output_attentions=True)\n",
    "\n",
    "if outputs_with_attention.attentions:\n",
    "    print(f\"\\n  - attentions: {len(outputs_with_attention.attentions)} layers\")\n",
    "    print(f\"    ê° attention shape: {outputs_with_attention.attentions[0].shape}\")\n",
    "    print(f\"    (batch_size, num_heads, seq_length, seq_length)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c83b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë§Œì•½, SequenceClassification ëª¨ë¸ì´ ì•„ë‹Œ ê²½ìš°?\n",
    "model_ = AutoModelForSequenceClassification.from_pretrained(\"klue/roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d5ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8e787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forwardê°€ ë ê¹Œ...?\n",
    "text = \"This movie is totally awful!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"ì…ë ¥ í…ìŠ¤íŠ¸: '{text}'\\n\")\n",
    "print(\"Forward ë©”ì„œë“œ ì…ë ¥ íŒŒë¼ë¯¸í„°:\")\n",
    "for key, value in inputs.items():\n",
    "    print(f\"  - {key}: shape {value.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model_(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0e6d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶œë ¥ì„ í™•ì¸í•´ë³´ì\n",
    "print(\"\\nForward ë©”ì„œë“œ ì¶œë ¥:\")\n",
    "print(f\"  - logits: shape {outputs.logits.shape}\")\n",
    "print(f\"    ê°’: {outputs.logits[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "causal_lm_detail",
   "metadata": {},
   "source": [
    "## 4.3 CausalLM ëª¨ë¸ ìƒì„¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5accd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 ëª¨ë¸ ë¡œë“œ\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf90e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ê°œìš”\n",
    "print(\"ëª¨ë¸ ê°œìš”:\")\n",
    "print(f\"- Model type: {model.config.model_type}\")\n",
    "print(f\"- Vocab size: {model.config.vocab_size}\")\n",
    "print(f\"- Max position embeddings: {model.config.n_positions}\")\n",
    "print(f\"- Number of layers: {model.config.n_layer}\")\n",
    "print(f\"- Hidden size: {model.config.hidden_size}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ìƒì„¸ êµ¬ì¡°\n",
    "print(\"ëª¨ë¸ ìƒì„¸ êµ¬ì¡°:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ forward pass\n",
    "prompt = \"The future of AI is going to be very\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"í”„ë¡¬í”„íŠ¸: '{prompt}'\\n\")\n",
    "print(f\"í”„ë¡¬í”„íŠ¸ í† í° ê¸¸ì´: {len(tokenizer.tokenize(prompt))}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶œë ¥ íŒŒë¼ë¯¸í„°\n",
    "print(outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f17945",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Forward ë©”ì„œë“œ ì¶œë ¥:\")\n",
    "print(f\"  - logits shape: {outputs.logits.shape}\")\n",
    "print(f\"    (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a67afef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ìŒ í† í° ì˜ˆì¸¡\n",
    "next_token_logits = outputs.logits[0, -1, :]  # ë§ˆì§€ë§‰ í† í°ì˜ logits\n",
    "next_token_probs = torch.softmax(next_token_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37930dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒìœ„ 5ê°œ ê°€ëŠ¥í•œ ë‹¤ìŒ í† í°\n",
    "top_k = 5\n",
    "top_probs, top_indices = torch.topk(next_token_probs, top_k)\n",
    "\n",
    "print(f\"\\në‹¤ìŒ í† í° ì˜ˆì¸¡ (Top {top_k}):\")\n",
    "for i in range(top_k):\n",
    "    token = tokenizer.decode(top_indices[i])\n",
    "    prob = top_probs[i].item()\n",
    "    print(f\"  {i+1}. '{token}' (í™•ë¥ : {prob:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "causal_lm_forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelsë¥¼ ì‚¬ìš©í•œ Loss ê³„ì‚°\n",
    "\n",
    "# ì…ë ¥ê³¼ ë™ì¼í•œ labels ìƒì„± (teacher forcing)\n",
    "prompt = \"The future of AI is going to be very interesting. With transformers, it will be even more interesting.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "labels = inputs['input_ids'].clone()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_with_loss = model(**inputs, labels=labels, outputs_with_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceeb2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê° ìœ„ì¹˜ì—ì„œì˜ Top-k í† í° ì˜ˆì¸¡\n",
    "\n",
    "next_tokens_logits = outputs_with_loss.logits[0, :, :]  # ë§ˆì§€ë§‰ í† í°ì˜ logits\n",
    "next_tokens_probs = torch.softmax(next_tokens_logits, dim=-1)\n",
    "\n",
    "top_k = 5\n",
    "top_probs, top_indices = torch.topk(next_tokens_probs, top_k)\n",
    "\n",
    "for i in range(len(next_tokens_logits)-1):\n",
    "    print(f\"í˜„ì¬ Input Text: {tokenizer.decode(inputs['input_ids'][0][:i+1])}\")\n",
    "    print(f\"ì‹¤ì œ ì •ë‹µ: {tokenizer.decode(labels[0][i+1])}\")\n",
    "    for j in range(top_k):\n",
    "        print(f\"  {i+1}. '{tokenizer.decode(top_indices[i][j])}' (í™•ë¥ : {top_probs[i][j]:.3f})\")\n",
    "    print(\"-\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf75498",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"  - Loss: {outputs_with_loss.loss.item():.4f}\")\n",
    "print(f\"  - Perplexity: {torch.exp(outputs_with_loss.loss).item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_comparison",
   "metadata": {},
   "source": [
    "## 4.4 ëª¨ë¸ í´ë˜ìŠ¤ ë¹„êµ ë° ì„ íƒ ê°€ì´ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_selection_guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š ëª¨ë¸ í´ë˜ìŠ¤ ì„ íƒ ê°€ì´ë“œ\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# íƒœìŠ¤í¬ë³„ ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ\n",
    "task_guide = {\n",
    "    \"ê°ì„± ë¶„ì„ / í…ìŠ¤íŠ¸ ë¶„ë¥˜\": {\n",
    "        \"class\": \"AutoModelForSequenceClassification\",\n",
    "        \"ì¶”ì²œ ëª¨ë¸\": [\"bert-base-uncased\", \"roberta-base\", \"distilbert-base-uncased\"],\n",
    "        \"ì¶œë ¥\": \"logits (num_labels ì°¨ì›)\",\n",
    "        \"ìš©ë„\": \"ìŠ¤íŒ¸ í•„í„°ë§, ê°ì„± ë¶„ì„, ì£¼ì œ ë¶„ë¥˜\"\n",
    "    },\n",
    "    \"ê°œì²´ëª… ì¸ì‹ / POS íƒœê¹…\": {\n",
    "        \"class\": \"AutoModelForTokenClassification\",\n",
    "        \"ì¶”ì²œ ëª¨ë¸\": [\"bert-base-cased\", \"roberta-base\"],\n",
    "        \"ì¶œë ¥\": \"ê° í† í°ë³„ logits\",\n",
    "        \"ìš©ë„\": \"NER, POS tagging, êµ¬ë¬¸ ë¶„ì„\"\n",
    "    },\n",
    "    \"í…ìŠ¤íŠ¸ ìƒì„±\": {\n",
    "        \"class\": \"AutoModelForCausalLM\",\n",
    "        \"ì¶”ì²œ ëª¨ë¸\": [\"gpt2\", \"gpt2-medium\", \"gpt2-large\"],\n",
    "        \"ì¶œë ¥\": \"ë‹¤ìŒ í† í° ì˜ˆì¸¡ logits\",\n",
    "        \"ìš©ë„\": \"ì±—ë´‡, ìŠ¤í† ë¦¬ ìƒì„±, ì½”ë“œ ìƒì„±\"\n",
    "    },\n",
    "    \"ì§ˆì˜ì‘ë‹µ\": {\n",
    "        \"class\": \"AutoModelForQuestionAnswering\",\n",
    "        \"ì¶”ì²œ ëª¨ë¸\": [\"bert-base-uncased\", \"roberta-base\"],\n",
    "        \"ì¶œë ¥\": \"start/end position logits\",\n",
    "        \"ìš©ë„\": \"ë¬¸ì„œ ë‚´ ë‹µë³€ ì¶”ì¶œ, FAQ ì‹œìŠ¤í…œ\"\n",
    "    },\n",
    "    \"ë¹ˆì¹¸ ì±„ìš°ê¸°\": {\n",
    "        \"class\": \"AutoModelForMaskedLM\",\n",
    "        \"ì¶”ì²œ ëª¨ë¸\": [\"bert-base-uncased\", \"roberta-base\"],\n",
    "        \"ì¶œë ¥\": \"ë§ˆìŠ¤í¬ í† í° ìœ„ì¹˜ì˜ vocab logits\",\n",
    "        \"ìš©ë„\": \"ë¬¸ì¥ ì™„ì„±, ë‹¨ì–´ ì˜ˆì¸¡\"\n",
    "    },\n",
    "}\n",
    "\n",
    "for task, info in task_guide.items():\n",
    "    print(f\"\\nğŸ“Œ {task}\")\n",
    "    print(f\"   í´ë˜ìŠ¤: {info['class']}\")\n",
    "    print(f\"   ì¶”ì²œ ëª¨ë¸: {', '.join(info['ì¶”ì²œ ëª¨ë¸'][:2])}\")\n",
    "    print(f\"   ì¶œë ¥ í˜•íƒœ: {info['ì¶œë ¥']}\")\n",
    "    print(f\"   ì‚¬ìš© ì˜ˆ: {info['ìš©ë„']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5_title",
   "metadata": {},
   "source": [
    "---\n",
    "# **5. ì¶”ë¡  ë° ê²°ê³¼ í™•ì¸**\n",
    "\n",
    "> **í•™ìŠµ ëª©í‘œ**:\n",
    "> - ë¡œì§“(logits) í•´ì„ ë° í›„ì²˜ë¦¬(softmax, argmax ë“±) ì´í•´\n",
    "> - ì‹¤ì œ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logits_concept",
   "metadata": {},
   "source": [
    "## 5.1 Logits ì´í•´ì™€ ë³€í™˜\n",
    "\n",
    "### **Logitsë€?**\n",
    "- ëª¨ë¸ì˜ ìµœì¢… ë ˆì´ì–´ ì¶œë ¥ (í™œì„±í™” í•¨ìˆ˜ ì ìš© ì „)\n",
    "- í™•ë¥ ì´ ì•„ë‹Œ ì›ì‹œ ì ìˆ˜(raw scores)\n",
    "- Softmaxë¥¼ í†µí•´ í™•ë¥ ë¡œ ë³€í™˜ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a2ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤\n",
    "test_sentences = [\n",
    "    \"This product is amazing!\",\n",
    "    \"Terrible experience.\",\n",
    "    \"I think\",\n",
    "]\n",
    "\n",
    "# í† í¬ë‚˜ì´ì§•\n",
    "inputs = tokenizer(test_sentences, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# ì¶”ë¡ \n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db206c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ì¶œë ¥ ì¢…ë¥˜: {outputs.keys()}\")\n",
    "print(f\"ì¶œë ¥ í¬ê¸°: {outputs.logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logits_processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence, logits in zip(test_sentences, outputs.logits):\n",
    "    print(f\"\\nì…ë ¥: '{sentence}'\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # 1. Raw logits\n",
    "    print(f\"1. Raw Logits: {logits.tolist()}\")\n",
    "    print(f\"   â†’ NEGATIVE={logits[0]:.3f}, POSITIVE={logits[1]:.3f}\")\n",
    "    \n",
    "    # 2. Softmax í™•ë¥ \n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    print(f\"\\n2. Softmax í™•ë¥ : {probs.tolist()}\")\n",
    "    print(f\"   â†’ NEGATIVE={probs[0]:.3%}, POSITIVE={probs[1]:.3%}\")\n",
    "    \n",
    "    # 3. Argmax ì˜ˆì¸¡\n",
    "    prediction = torch.argmax(logits)\n",
    "    labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "    print(f\"\\n3. ìµœì¢… ì˜ˆì¸¡: {labels[prediction]} (ì¸ë±ìŠ¤: {prediction})\")\n",
    "    \n",
    "    # 4. Confidence score\n",
    "    confidence = probs.max().item()\n",
    "    print(f\"\\n4. Confidence: {confidence:.3%}\")\n",
    "    print(\"\\n\")\n",
    "    print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch_inference",
   "metadata": {},
   "source": [
    "## 5.2 ë°°ì¹˜ ì¶”ë¡ ê³¼ ìµœì í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f5224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì¤€ë¹„\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±\n",
    "test_texts = [\n",
    "    \"Great product, highly recommend! Great product, highly recommend!Great product, highly recommend!Great product, highly recommend!Great product, highly recommend!\",\n",
    "    \"Not worth the price.\",\n",
    "    \"Average quality, nothing special.\",\n",
    "    \"Exceeded my expectations!\",\n",
    "    \"Disappointed with the purchase.\",\n",
    "    \"Good value for money.\",\n",
    "    \"Would buy again.\",\n",
    "    \"Complete waste of money.\"\n",
    "] * 5  # 40ê°œ ë¬¸ì¥\n",
    "\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_texts)}ê°œ ë¬¸ì¥\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch_inference_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"âš¡ ë°°ì¹˜ ì¶”ë¡  ì„±ëŠ¥ ë¹„êµ\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. ê°œë³„ ì¶”ë¡ \n",
    "print(\"1. ê°œë³„ ì¶”ë¡  (one-by-one):\")\n",
    "start_time = time.time()\n",
    "\n",
    "individual_results = []\n",
    "for text in test_texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = F.softmax(outputs.logits, dim=-1)\n",
    "    individual_results.append(probs)\n",
    "\n",
    "individual_time = time.time() - start_time\n",
    "print(f\"   ì†Œìš” ì‹œê°„: {individual_time:.3f}ì´ˆ\")\n",
    "print(f\"   í‰ê·  ì²˜ë¦¬ ì‹œê°„: {individual_time/len(test_texts)*1000:.2f}ms/ë¬¸ì¥\")\n",
    "\n",
    "# 2. ë°°ì¹˜ ì¶”ë¡ \n",
    "print(\"\\n2. ë°°ì¹˜ ì¶”ë¡  (batch processing):\")\n",
    "start_time = time.time()\n",
    "\n",
    "# ë°°ì¹˜ë¡œ í† í¬ë‚˜ì´ì§•\n",
    "inputs = tokenizer(\n",
    "    test_texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# ë°°ì¹˜ ì¶”ë¡ \n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "batch_probs = F.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "batch_time = time.time() - start_time\n",
    "print(f\"   ì†Œìš” ì‹œê°„: {batch_time:.3f}ì´ˆ\")\n",
    "print(f\"   í‰ê·  ì²˜ë¦¬ ì‹œê°„: {batch_time/len(test_texts)*1000:.2f}ms/ë¬¸ì¥\")\n",
    "\n",
    "# ì„±ëŠ¥ ë¹„êµ\n",
    "print(f\"\\nğŸ“Š ì„±ëŠ¥ í–¥ìƒ: {individual_time/batch_time:.2f}ë°° ë¹ ë¦„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "# **ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤! ğŸ‰**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "industrial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
