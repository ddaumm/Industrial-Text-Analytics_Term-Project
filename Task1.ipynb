{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da73f216",
   "metadata": {},
   "source": [
    "#### 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e616e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8daeac4",
   "metadata": {},
   "source": [
    "#### Method 정의\n",
    "* retrieval에서, query에 대한 키워드 추출 및 query_filter에서 이를 사용하도록 하는 기능 추가!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70ca13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(documents_df_, chunk_size_, chunk_overlap_):\n",
    "    \"\"\"\n",
    "    chunk_size_, chunk_overlap_에 따라, documents_df_의 텍스트를 Chunking\n",
    "    \"\"\"\n",
    "    chunk_results = []\n",
    "    chunk_idx = 0\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size_,\n",
    "        chunk_overlap=chunk_overlap_,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    for idx, row in documents_df_.iterrows():\n",
    "        origin_text = row['summary']\n",
    "\n",
    "        chunks = text_splitter.create_documents([origin_text])\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk_results.append({\n",
    "                'chunk_id':chunk_idx,\n",
    "                'text':chunk.page_content,\n",
    "                'metadata':{\n",
    "                    'category':row['category'],\n",
    "                    'press':row['press'],\n",
    "                    'title':row['title'],\n",
    "                    'chunk_size':len(chunk.page_content)\n",
    "                }\n",
    "            })\n",
    "            chunk_idx += 1\n",
    "\n",
    "    return chunk_results\n",
    "\n",
    "def embedding_indexing(chunk_results_, embedding_model_name_, collection_name_, use_instruct_prefix=False):\n",
    "    # Chunking\n",
    "    chunk_results = chunk_results_\n",
    "\n",
    "    # Retriever 모델 선택 및 임베딩\n",
    "    embedding_model = SentenceTransformer(embedding_model_name_)\n",
    "\n",
    "    EMBEDDING_DIM = embedding_model.get_sentence_embedding_dimension() # 임베딩 차원 확인\n",
    "\n",
    "    chunk_texts = [chunk['text'] for chunk in chunk_results] # 청크 텍스트 리스트\n",
    "\n",
    "    if use_instruct_prefix:\n",
    "        chunk_texts = [f\"passage: {chunk_text}\" for chunk_text in chunk_texts]\n",
    "\n",
    "    embeddings = embedding_model.encode(chunk_texts, show_progress_bar=True) # 임베딩 / 임베딩 과정 시각화\n",
    "\n",
    "    # Qdrant 컬렉션 구축\n",
    "    client = QdrantClient(host='localhost', port=6333)\n",
    "\n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name_,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=EMBEDDING_DIM,\n",
    "            distance=models.Distance.COSINE # 코사인 유사도\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Indexing: Qdrant에 포인트 삽입\n",
    "    points = []\n",
    "\n",
    "    # Qdrant에 삽입할 PointStruct 리스트 생성\n",
    "    for idx, chunk in enumerate(chunk_results):\n",
    "        # metadata를 Payload로 사용, 원문 'text'를 포함\n",
    "        payload_data = chunk['metadata']\n",
    "        payload_data['text'] = chunk['text']\n",
    "\n",
    "        # PointStruct 생성: id는 chunk_id(0부터 시작) 사용\n",
    "        points.append(\n",
    "            models.PointStruct(\n",
    "                id=chunk['chunk_id'], # 청크 ID를 Qdrant의 고유 ID로 사용\n",
    "                vector=embeddings[idx].tolist(), # numpy 배열을 list로 변환하여 삽입\n",
    "                payload=payload_data\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 데이터 삽입(upsert): Batch 단위로 삽입\n",
    "    BATCH_SIZE = 1000\n",
    "\n",
    "    for i in range(0, len(points), BATCH_SIZE):\n",
    "        batch_points = points[i:i+BATCH_SIZE]\n",
    "\n",
    "        try:\n",
    "            operation_info = client.upsert(\n",
    "                collection_name=collection_name_,\n",
    "                wait=True,\n",
    "                points=batch_points\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: 배치 {int(i/BATCH_SIZE)} 삽입 실패: {e}\")\n",
    "            \n",
    "            return\n",
    "        \n",
    "    # 정상 작동 확인\n",
    "    collection_info = client.count(\n",
    "        collection_name=collection_name_,\n",
    "        exact=True\n",
    "    )\n",
    "\n",
    "    if collection_info.count == len(points):\n",
    "        print(f\"임베딩 및 인덱싱 정상적으로 실행 완료 됨!\")\n",
    "\n",
    "def retrieval(embedding_model_name: str, collection_name: str, query: str, top_k: int, use_instruct_prefix=False):\n",
    "    \"\"\"\n",
    "    Query를 임베딩하고, Qdrant 컬렉션에서 관련 문서를 검색.\n",
    "    \"\"\"\n",
    "\n",
    "    # 임베딩 모델 로드\n",
    "    embedding_model = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "    # 쿼리 임베딩\n",
    "    if use_instruct_prefix: # Instruct 모델을 사용하는 경우 'query: ' 접두사 추가\n",
    "        query = f\"query: {query}\"\n",
    "\n",
    "    query_vector = embedding_model.encode(query).tolist()\n",
    "\n",
    "    # Qdrant Client 로드\n",
    "    client = QdrantClient(host='localhost', port=6333)\n",
    "\n",
    "    # Qdrant 검색 수행\n",
    "    search_results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_vector,\n",
    "        limit=top_k,\n",
    "        with_payload=True, # 검색된 포인트의 메타 데이터\n",
    "        with_vectors=False # 검색된 포인트의 임베딩 벡터 (필요 X)\n",
    "    )\n",
    "\n",
    "    # 결과 출력\n",
    "    if not search_results:\n",
    "        print(f\"ERROR: 검색 결과가 없습니다.\")\n",
    "        \n",
    "        return\n",
    "\n",
    "    \n",
    "    for rank, result in enumerate(search_results.points):\n",
    "        payload = result.payload\n",
    "\n",
    "        chunk_text = payload.get('text', '텍스트 없음')\n",
    "        source_title = payload.get('title', '제목 없음')\n",
    "        source_press = payload.get('press', '출처 없음')\n",
    "\n",
    "        print(f\"[{rank+1}]: \\nchunk_text: {chunk_text}\\nsource_title: {source_title}\\nsource_press: {source_press}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e794d",
   "metadata": {},
   "source": [
    "#### 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2ba682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "dataset_id = 'daekeun-ml/naver-news-summarization-ko'\n",
    "dataset = load_dataset(dataset_id, split='test')\n",
    "documents_df = dataset.to_pandas()\n",
    "\n",
    "# Example\n",
    "print(f\"문서 수 : {len(documents_df)}\")\n",
    "print(f\"문서 Columns : {documents_df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a4bcec",
   "metadata": {},
   "source": [
    "#### CHUNKING -> EMBEDDING -> INDEXING\n",
    "* Chunk Size, Chunk Overlap, Embedding Model, Collection Name 확인 필수!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679dbf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 설정\n",
    "CHUNK_PARAMS = [(100, 20)]\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"xlm-r-large-en-ko-nli-ststb\"\n",
    "USE_INSTRUCT_PREFIX = False\n",
    "\n",
    "for CHUNK_SIZE, CHUNK_OVERLAP in CHUNK_PARAMS:\n",
    "    COLLECTION_NAME = f'XLM_{CHUNK_SIZE}'\n",
    "\n",
    "    # log\n",
    "    print(f\"{COLLECTION_NAME} 컬렉션 생성 시작\")\n",
    "\n",
    "    chunk_results = chunking(documents_df, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "\n",
    "    # log\n",
    "    print(f\"청킹 완료\")\n",
    "\n",
    "    embedding_indexing(chunk_results, EMBEDDING_MODEL_NAME, COLLECTION_NAME, USE_INSTRUCT_PREFIX)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1256a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 설정\n",
    "CHUNK_PARAMS = [(100, 20)]\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"intfloat/multilingual-e5-large-instruct\"\n",
    "USE_INSTRUCT_PREFIX = True\n",
    "\n",
    "for CHUNK_SIZE, CHUNK_OVERLAP in CHUNK_PARAMS:\n",
    "    COLLECTION_NAME = f'ME5_{CHUNK_SIZE}'\n",
    "\n",
    "    # log\n",
    "    print(f\"{COLLECTION_NAME} 컬렉션 생성 시작\")\n",
    "\n",
    "    chunk_results = chunking(documents_df, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "\n",
    "    # log\n",
    "    print(f\"청킹 완료\")\n",
    "\n",
    "    embedding_indexing(chunk_results, EMBEDDING_MODEL_NAME, COLLECTION_NAME, USE_INSTRUCT_PREFIX)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a8e774",
   "metadata": {},
   "source": [
    "#### Retrieval: Query -> Top K개의 Chunk 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"xlm-r-large-en-ko-nli-ststb\"\n",
    "COLLECTION_NAME = 'XLM_500'\n",
    "USE_INSTRUCT_PREFIX = True\n",
    "QUERY = '차량용 복합기능형 졸음 방지 단말기의 구성 요소 중 운전자의 얼굴 영상을 촬영하는 장치는 무엇입니까?'\n",
    "TOP_K = 3\n",
    "\n",
    "retrieval(EMBEDDING_MODEL_NAME, COLLECTION_NAME, QUERY, TOP_K, USE_INSTRUCT_PREFIX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
