{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da73f216",
   "metadata": {},
   "source": [
    "#### 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e616e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8daeac4",
   "metadata": {},
   "source": [
    "#### Method 정의\n",
    "* retrieval에서, query에 대한 키워드 추출 및 query_filter에서 이를 사용하도록 하는 기능 추가!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a70ca13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(documents_df_, chunk_size_, chunk_overlap_):\n",
    "    \"\"\"\n",
    "    chunk_size_, chunk_overlap_에 따라, documents_df_의 텍스트를 Chunking\n",
    "    \"\"\"\n",
    "    chunk_results = []\n",
    "    chunk_idx = 0\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size_,\n",
    "        chunk_overlap=chunk_overlap_,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    for idx, row in documents_df_.iterrows():\n",
    "        origin_text = row['summary']\n",
    "\n",
    "        chunks = text_splitter.create_documents([origin_text])\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk_results.append({\n",
    "                'chunk_id':chunk_idx,\n",
    "                'text':chunk.page_content,\n",
    "                'metadata':{\n",
    "                    'category':row['category'],\n",
    "                    'press':row['press'],\n",
    "                    'title':row['title'],\n",
    "                    'chunk_size':len(chunk.page_content)\n",
    "                }\n",
    "            })\n",
    "            chunk_idx += 1\n",
    "\n",
    "    return chunk_results\n",
    "\n",
    "def embedding_indexing(chunk_results_, embedding_model_name_, collection_name_, use_instruct_prefix=False):\n",
    "    # Chunking\n",
    "    chunk_results = chunk_results_\n",
    "\n",
    "    # Retriever 모델 선택 및 임베딩\n",
    "    embedding_model = SentenceTransformer(embedding_model_name_)\n",
    "\n",
    "    EMBEDDING_DIM = embedding_model.get_sentence_embedding_dimension() # 임베딩 차원 확인\n",
    "\n",
    "    chunk_texts = [chunk['text'] for chunk in chunk_results] # 청크 텍스트 리스트\n",
    "\n",
    "    if use_instruct_prefix:\n",
    "        chunk_texts = [f\"passage: {chunk_text}\" for chunk_text in chunk_texts]\n",
    "\n",
    "    embeddings = embedding_model.encode(chunk_texts, show_progress_bar=True) # 임베딩 / 임베딩 과정 시각화\n",
    "\n",
    "    # Qdrant 컬렉션 구축\n",
    "    client = QdrantClient(host='localhost', port=6333)\n",
    "\n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name_,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=EMBEDDING_DIM,\n",
    "            distance=models.Distance.COSINE # 코사인 유사도\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Indexing: Qdrant에 포인트 삽입\n",
    "    points = []\n",
    "\n",
    "    # Qdrant에 삽입할 PointStruct 리스트 생성\n",
    "    for idx, chunk in enumerate(chunk_results):\n",
    "        # metadata를 Payload로 사용, 원문 'text'를 포함\n",
    "        payload_data = chunk['metadata']\n",
    "        payload_data['text'] = chunk['text']\n",
    "\n",
    "        # PointStruct 생성: id는 chunk_id(0부터 시작) 사용\n",
    "        points.append(\n",
    "            models.PointStruct(\n",
    "                id=chunk['chunk_id'], # 청크 ID를 Qdrant의 고유 ID로 사용\n",
    "                vector=embeddings[idx].tolist(), # numpy 배열을 list로 변환하여 삽입\n",
    "                payload=payload_data\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 데이터 삽입(upsert): Batch 단위로 삽입\n",
    "    BATCH_SIZE = 1000\n",
    "\n",
    "    for i in range(0, len(points), BATCH_SIZE):\n",
    "        batch_points = points[i:i+BATCH_SIZE]\n",
    "\n",
    "        try:\n",
    "            operation_info = client.upsert(\n",
    "                collection_name=collection_name_,\n",
    "                wait=True,\n",
    "                points=batch_points\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: 배치 {int(i/BATCH_SIZE)} 삽입 실패: {e}\")\n",
    "            \n",
    "            return\n",
    "        \n",
    "    # 정상 작동 확인\n",
    "    collection_info = client.count(\n",
    "        collection_name=collection_name_,\n",
    "        exact=True\n",
    "    )\n",
    "\n",
    "    if collection_info.count == len(points):\n",
    "        print(f\"임베딩 및 인덱싱 정상적으로 실행 완료 됨!\")\n",
    "\n",
    "def retrieval(embedding_model_name: str, collection_name: str, query: str, top_k: int, use_instruct_prefix=False):\n",
    "    \"\"\"\n",
    "    Query를 임베딩하고, Qdrant 컬렉션에서 관련 문서를 검색.\n",
    "    \"\"\"\n",
    "\n",
    "    # 임베딩 모델 로드\n",
    "    embedding_model = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "    # 쿼리 임베딩\n",
    "    if use_instruct_prefix: # Instruct 모델을 사용하는 경우 'query: ' 접두사 추가\n",
    "        query = f\"query: {query}\"\n",
    "\n",
    "    query_vector = embedding_model.encode(query).tolist()\n",
    "\n",
    "    # Qdrant Client 로드\n",
    "    client = QdrantClient(host='localhost', port=6333)\n",
    "\n",
    "    # Qdrant 검색 수행\n",
    "    search_results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_vector,\n",
    "        limit=top_k,\n",
    "        with_payload=True, # 검색된 포인트의 메타 데이터\n",
    "        with_vectors=False # 검색된 포인트의 임베딩 벡터 (필요 X)\n",
    "    )\n",
    "\n",
    "    # 결과 출력\n",
    "    if not search_results:\n",
    "        print(f\"ERROR: 검색 결과가 없습니다.\")\n",
    "        \n",
    "        return\n",
    "\n",
    "    \n",
    "    for rank, result in enumerate(search_results.points):\n",
    "        payload = result.payload\n",
    "\n",
    "        chunk_text = payload.get('text', '텍스트 없음')\n",
    "        source_title = payload.get('title', '제목 없음')\n",
    "        source_press = payload.get('press', '출처 없음')\n",
    "\n",
    "        print(f\"[{rank+1}]: \\nchunk_text: {chunk_text}\\nsource_title: {source_title}\\nsource_press: {source_press}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e794d",
   "metadata": {},
   "source": [
    "#### 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2ba682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 수 : 2740\n",
      "문서 Columns : Index(['date', 'category', 'press', 'title', 'document', 'link', 'summary'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 로드\n",
    "dataset_id = 'daekeun-ml/naver-news-summarization-ko'\n",
    "dataset = load_dataset(dataset_id, split='test')\n",
    "documents_df = dataset.to_pandas()\n",
    "\n",
    "# Example\n",
    "print(f\"문서 수 : {len(documents_df)}\")\n",
    "print(f\"문서 Columns : {documents_df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a4bcec",
   "metadata": {},
   "source": [
    "#### CHUNKING -> EMBEDDING -> INDEXING\n",
    "* Chunk Size, Chunk Overlap, Embedding Model, Collection Name 확인 필수!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "679dbf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLM_100 컬렉션 생성 시작\n",
      "청킹 완료\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0740be46169d4654b160d7a81dfbd916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dlaek\\AppData\\Local\\Temp\\ipykernel_11920\\1190063013.py:53: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 및 인덱싱 정상적으로 실행 완료 됨!\n"
     ]
    }
   ],
   "source": [
    "# 변수 설정\n",
    "CHUNK_PARAMS = [(100, 20)]\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"xlm-r-large-en-ko-nli-ststb\"\n",
    "USE_INSTRUCT_PREFIX = False\n",
    "\n",
    "for CHUNK_SIZE, CHUNK_OVERLAP in CHUNK_PARAMS:\n",
    "    COLLECTION_NAME = f'XLM_{CHUNK_SIZE}'\n",
    "\n",
    "    # log\n",
    "    print(f\"{COLLECTION_NAME} 컬렉션 생성 시작\")\n",
    "\n",
    "    chunk_results = chunking(documents_df, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "\n",
    "    # log\n",
    "    print(f\"청킹 완료\")\n",
    "\n",
    "    embedding_indexing(chunk_results, EMBEDDING_MODEL_NAME, COLLECTION_NAME, USE_INSTRUCT_PREFIX)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1256a5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ME5_100 컬렉션 생성 시작\n",
      "청킹 완료\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b782680109304281b91331a32053f278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dlaek\\AppData\\Local\\Temp\\ipykernel_11920\\1190063013.py:53: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 및 인덱싱 정상적으로 실행 완료 됨!\n"
     ]
    }
   ],
   "source": [
    "# 변수 설정\n",
    "CHUNK_PARAMS = [(100, 20)]\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"intfloat/multilingual-e5-large-instruct\"\n",
    "USE_INSTRUCT_PREFIX = True\n",
    "\n",
    "for CHUNK_SIZE, CHUNK_OVERLAP in CHUNK_PARAMS:\n",
    "    COLLECTION_NAME = f'ME5_{CHUNK_SIZE}'\n",
    "\n",
    "    # log\n",
    "    print(f\"{COLLECTION_NAME} 컬렉션 생성 시작\")\n",
    "\n",
    "    chunk_results = chunking(documents_df, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "\n",
    "    # log\n",
    "    print(f\"청킹 완료\")\n",
    "\n",
    "    embedding_indexing(chunk_results, EMBEDDING_MODEL_NAME, COLLECTION_NAME, USE_INSTRUCT_PREFIX)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a8e774",
   "metadata": {},
   "source": [
    "#### Retrieval: Query -> Top K개의 Chunk 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487f4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]: \n",
      "chunk_text: 아이일, 아이트로닉스는 차량용 복합기능형 졸음 방지 단말기 특허를 출원했다고 4일 밝혔으며 신규 특허는 자동차 주행 중 운전자의 졸음운전을 방지하는 상태 검출 기술에 관한 것으로, 해당 단말기는 가시광선 및 근적외선 광원을 조사하는 광원 모듈 운전자의 얼굴 영상을 촬영하는 가시광선 및 근적외선 카메라 차량 실내의 이산화탄소 농도를 측정하는 이산화탄소 센서로 구성됐다.\n",
      "source_title: 아이트로닉스 차량용 복합기능형 졸음 방지 단말기 특허 출원\n",
      "source_press: 머니투데이 \n",
      "[2]: \n",
      "chunk_text: 악사 AXA 손해보험이 차량 출고 시 장착된 단말기로 차량의 실시간 주행 정보는 물론 운전자의 평소 운전 습관이나 차량의 사고 이력까지 투명하게 확인하고 차량 출고 시 장착된 단말기를 이용해 차량의 실시간 주행 정보는 물론 운전자의 평소 운전 습관이나 차량의 사고 이력까지 투명하게 확인하는 AXA커넥티드카 안전운전 할인 자동차보험 특별 약관을 신설했다고 4일 밝혔다.\n",
      "source_title: 악사손보 안전운전하면 보험료 깎아드려요\n",
      "source_press: 전자신문 \n",
      "[3]: \n",
      "chunk_text: 6스트 GIST은 레벨4 기술의 자율주행 자동차가 도로 위의 경찰 수신호나 지시봉을 인식하기위한 세계 최대 규모의 수신호 데이터 데이터베이스 DB 를 구축해 이를 통해 자동차가 교통 수신호를 인식하고 정지하는 시연에 성공했다고 6일 밝혔으며 이를 통해 구축된 경찰 수신호 도로주행 이미지 보행자 및 경찰관 추적용 이미지 등의 데이터베이스는 향후 레벨 4 기술 이상의 자율주행 차량에 필수 요소인 교통 수신호 인지의 토대를 마련할 것으로 기대되며 이번 연구를 통해 구축된 경찰 수신호 도로주행 이미지 보행자 및 경찰관 추적용 이미지 등의 데이터베이스는 향후 레벨 4 기술 이상의 자율주행 차량에 필수 요소인 교통 수신호 인지의 토대를 마련할 것으로 기대된다.\n",
      "source_title: 자율주행차가 교통경찰 수신호에 즉각 반응… 지스트 DB 구축·시연 성공\n",
      "source_press: 디지털데일리 \n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_MODEL_NAME = \"xlm-r-large-en-ko-nli-ststb\"\n",
    "COLLECTION_NAME = 'XLM_500'\n",
    "USE_INSTRUCT_PREFIX = True\n",
    "QUERY = '차량용 복합기능형 졸음 방지 단말기의 구성 요소 중 운전자의 얼굴 영상을 촬영하는 장치는 무엇입니까?'\n",
    "TOP_K = 3\n",
    "\n",
    "retrieval(EMBEDDING_MODEL_NAME, COLLECTION_NAME, QUERY, TOP_K, USE_INSTRUCT_PREFIX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
