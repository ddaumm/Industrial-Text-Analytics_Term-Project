{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e9c53b9",
   "metadata": {},
   "source": [
    "#### 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3541355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GEMINII_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(client, llm_model_params, query:str, search_results: List[Dict]):\n",
    "    \"\"\"\n",
    "    Retrieval 결과와 Query를 바탕으로 답변을 생성\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질문\n",
    "        search_results (List[Dict]): Qdrant 검색 결과 (payload) 포함\n",
    "    \"\"\"\n",
    "\n",
    "    # Context 조합\n",
    "    context_list = []\n",
    "\n",
    "    for rank, result in enumerate(search_results):\n",
    "        context_text = result.payload.get('text', 'no text')\n",
    "        context_list.append(f\"[{rank+1} {context_text}]\")\n",
    "\n",
    "    context = \"\\n--\\n\".join(context_list)\n",
    "\n",
    "    # 프롬프팅 전략\n",
    "    # System Instruction: Role Prompting\n",
    "    system_prompt = (\n",
    "        \"당신은 주어진 Context에 기반하여 사용자의 질문에 정확하고 간결하게 답변하는 전문가입니다.\"\n",
    "        \"답변은 오직 Context에 있는 정보만 사용해야 합니다. 만약 Context에서 질문에 대한 답을 찾을 수 없다면, '관련 정보를 찾을 수 없음'이라고만 답변하세요.\"\n",
    "    )\n",
    "\n",
    "    # User Prompt: CoT & 출처 명시 형식 요구\n",
    "    user_prompt = (\n",
    "        \"주어진 Context를 분석하여 다음 Query에 답변하는 단계를 '추론' 섹션에 작성하고, 최종 답변을 '답변' 섹션에 작성하세요. 최종 답변에는 참조한 문맥의 번호([N])를 반드시 명시해야 합니다.\"\n",
    "        f\"문맥(Context):\\n{context}\\n\"\n",
    "        f\"질문(Query):\\n{query}\\n\"\n",
    "        f\"출력 형식: \\n\"\n",
    "        f\"추론: [답변 도출 과정]\\n\"\n",
    "        f\"답변: [최종 답변 내용(반드시 출처 번호 명시)]\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=llm_model_params['model'],\n",
    "            contents=[\n",
    "                {'role': 'user', 'parts': [{'text': user_prompt}]}\n",
    "            ],\n",
    "            config={\n",
    "                'system_instruction': system_prompt,\n",
    "                'temperature': llm_model_params['temperature'],\n",
    "                'max_output_tokens': llm_model_params['max_tokens']\n",
    "            }\n",
    "        )\n",
    "        return response.text\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Gemini API 답변 생성 중 오류 발생: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7077cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini API 클라이언트 설정\n",
    "client = genai.Client(api_key=GEMINII_API_KEY)\n",
    "\n",
    "LLM_MODEL_PARAMS = {\n",
    "    'temperature':0.2,\n",
    "    'max_tokens':12,\n",
    "    'model':'gemini-2.5-flash'\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
